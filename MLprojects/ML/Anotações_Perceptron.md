
---

# üß† Anota√ß√µes Corrigidas: Perceptron e Redes Neurais

As **Redes Neurais Artificiais (RNA)** s√£o modelos de aprendizado **supervisionado** (em sua maioria) inspirados na estrutura biol√≥gica do c√©rebro. O Perceptron √© a forma mais elementar dessas redes.

## 1. O Perceptron (Single-Layer)

O Perceptron √© um **classificador linear bin√°rio**. Ele √© conceitualmente focado em problemas **linearmente separ√°veis** (onde uma linha reta pode dividir as classes). Se o problema n√£o for linear (como o problema do XOR), um √∫nico Perceptron n√£o consegue resolv√™-lo.

### Estrutura do Modelo

* **Entradas ():** As caracter√≠sticas (*features*) dos dados.
* **Pesos ():** Valores que ponderam a import√¢ncia de cada entrada.
* **Bias ():** Um termo extra que permite deslocar a fun√ß√£o de ativa√ß√£o para cima ou para baixo, garantindo que o neur√¥nio possa aprender mesmo quando as entradas s√£o zero.
* **Jun√ß√£o Sumadora:** Realiza o c√°lculo: .

---

## 2. Fun√ß√µes de Ativa√ß√£o

A fun√ß√£o de ativa√ß√£o decide a sa√≠da do neur√¥nio com base no resultado da soma.

* **Degrau (Step Function):** Sa√≠da determin√≠stica entre 0 ou 1.
* `if soma >= 0: return 1 else: return 0`


* **Tangente Hiperb√≥lica (tanh):** Escala a sa√≠da entre -1 e 1. Embora seja mais robusta, no Perceptron de camada √∫nica ela ainda s√≥ resolve divis√µes lineares.



---

## 3. O Processo de Aprendizado

Diferente do que se pensa, o Perceptron √© **determin√≠stico**: para o mesmo peso e entrada, a sa√≠da nunca muda. O "caos" aparente vem da inicializa√ß√£o aleat√≥ria dos pesos.

1. **Predi√ß√£o:** O dado entra, a soma √© feita e a fun√ß√£o de ativa√ß√£o gera um resultado.
2. **C√°lculo do Erro:** Compara√ß√£o com o "Or√°culo" (valor real).
* 


3. **Regra Delta (Ajuste):** Os pesos s√£o atualizados proporcionalmente ao erro e √† taxa de aprendizado ():
* 


4. **√âpoca:** Uma passagem completa por todo o conjunto de dados.

---

## 4. Diferen√ßas Cruciais de Arquitetura

| Caracter√≠stica | Perceptron Simples | Multi-Layer Perceptron (MLP) |
| --- | --- | --- |
| **Camadas** | Apenas Entrada e Sa√≠da | Entrada, Camadas Ocultas e Sa√≠da |
| **Problemas** | Apenas Linearmente Separ√°veis | Problemas N√£o-Lineares Complexos |
| **Algoritmo** | Regra Delta | **Backpropagation** |

> **Nota:** O **Backpropagation** √© o algoritmo que permite o erro "voltar" atrav√©s das camadas ocultas para ajustar pesos que n√£o est√£o conectados diretamente √† sa√≠da.

---

## 5. Implementa√ß√£o (Python)

```python
class Perceptron:
    def __init__(self, taxa_aprendizado=0.1, epocas=100):
        self._taxa = taxa_aprendizado
        self._epocas = epocas
        self.pesos = None
        self.bias = 0.0

    def ativador(self, soma):
        return 1 if soma >= 0 else 0

    def treinar(self, X, y):
        # Inicializa pesos com zero ou valores aleat√≥rios pequenos
        self.pesos = [0.0] * len(X[0])
        
        for _ in range(self._epocas):
            erros_na_epoca = 0
            for xi, target in zip(X, y):
                # Soma ponderada: sum(w * x) + b
                soma_ponderada = sum(w * x for w, x in zip(self.pesos, xi)) + self.bias
                previsao = self.ativador(soma_ponderada)
                
                erro = target - previsao
                if erro != 0:
                    # Atualiza√ß√£o dos pesos e bias (Regra Delta)
                    for i in range(len(self.pesos)):
                        self.pesos[i] += self._taxa * erro * xi[i]
                    self.bias += self._taxa * erro
                    erros_na_epoca += 1
            
            if erros_na_epoca == 0:
                break # Converg√™ncia atingida

```

---
